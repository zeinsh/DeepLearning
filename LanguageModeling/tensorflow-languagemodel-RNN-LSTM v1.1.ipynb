{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change generator to split text more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lib.dictionarymd import Dictionary\n",
    "from lib.textprocessingmd import convert_text\n",
    "from lib.fileoperationmd import getFilesFromPath,readTxtFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    DATAPATH='./data/shakespeare/'\n",
    "    VOCPATH='./preprocessed'\n",
    "    VOCFILE='articles.voc'\n",
    "\n",
    "    MAXLENGTH=1000  # MAX Length of the document\n",
    "    K=3 # beam search\n",
    "    n_hidden_rnn=200\n",
    "    embedding_dim=200\n",
    "    \n",
    "    batch_size = 1\n",
    "    n_epochs = 500\n",
    "    learning_rate = 0.01\n",
    "    learning_rate_decay = 1#.41\n",
    "    dropout_keep_probability = 0.6\n",
    "\n",
    "hp=HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames=getFilesFromPath(hp.DATAPATH)\n",
    "dataset=[]\n",
    "text=\"\"\n",
    "for fn in filenames:\n",
    "    ntext=convert_text(readTxtFromFile(hp.DATAPATH,fn))\n",
    "    text+=ntext\n",
    "    dataset.append(ntext)\n",
    "    break\n",
    "\n",
    "dictionary=Dictionary()\n",
    "dictionary.make_vocab(text,hp.VOCPATH,hp.VOCFILE)\n",
    "dictionary.load_vocab(hp.VOCPATH,hp.VOCFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I can understand from this code that multiple batches maybe with different maxlen\n",
    "def batches_generator(batch_size, docs,dictionary,\n",
    "                      shuffle=False, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(docs)\n",
    "    vecs=[dictionary.text2vec(doc)[:hp.MAXLENGTH] for doc in docs]\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        lengths=[len(s) for s in vecs]\n",
    "        order = np.argsort(lengths)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(vecs[idx][:-1])\n",
    "            y_list.append(vecs[idx][1:])\n",
    "            max_len_token = max(max_len_token, len(vecs[idx]))  #why?!\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * dictionary.word2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * dictionary.word2idx['<PAD>']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        \n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modifications\n",
    "# 1. add training/inference variable\n",
    "# 2. define placeholder for state [NONE,2*stateLength]\n",
    "# 3. if inference use state placeholder\n",
    "# 4. modify beam search\n",
    "# 5. try to use built-in beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zein/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "    self.initial_state_h= tf.placeholder(dtype=tf.float32, shape=[None, hp.n_hidden_rnn], name='initial_state_h')\n",
    "    self.initial_state_c= tf.placeholder(dtype=tf.float32, shape=[None, hp.n_hidden_rnn], name='initial_state_c')\n",
    "    \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, dtype=tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, dtype=tf.float32, name='embeddings_matrix')\n",
    "\n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    \n",
    "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn),\n",
    "                                                  input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "    #backward_cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                                                  \n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "\n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    rnn_output, self.states = tf.nn.dynamic_rnn(cell=forward_cell,\n",
    "                                      initial_state=tf.nn.rnn_cell.LSTMStateTuple(self.initial_state_c, self.initial_state_h),\n",
    "                                      sequence_length=self.lengths,\n",
    "                                      dtype=tf.float32,\n",
    "                                      inputs=embeddings)\n",
    "    #rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "    \n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, vocabulary_size, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    self.softmax_output = tf.nn.softmax(logits=self.logits)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    self.predictions = tf.argmax(self.softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(self, vocabulary_size, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, vocabulary_size)\n",
    "    #loss_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=ground_truth_tags_one_hot)\n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), dtype=tf.float32)\n",
    "    loss_tensor = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits=self.logits,\n",
    "        targets=self.ground_truth_tags,\n",
    "        weights=mask)\n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    # Be careful that the argument of tf.reduce_mean should be\n",
    "    # multiplication of mask and loss_tensor.\n",
    "    self.loss = tf.reduce_mean(loss_tensor)#np.multiply(mask, loss_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, dtype=tf.float32)  ##??\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.initial_state_h: np.zeros((lengths.shape[0],hp.n_hidden_rnn)),\n",
    "                 self.initial_state_c: np.zeros((lengths.shape[0],hp.n_hidden_rnn)),\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch,init_c,init_h):\n",
    "    lengths=np.array([1000000]*len(x_batch))\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.initial_state_h: init_h,\n",
    "                 self.initial_state_c: init_c,\n",
    "                 self.lengths: lengths}\n",
    "    k=3\n",
    "    softmax, states = session.run([self.softmax_output ,self.states], feed_dict=feed_dict)\n",
    "    return softmax, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zein/.local/lib/python3.5/site-packages/tensorflow/python/ops/clip_ops.py:113: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = LSTMModel(vocabulary_size=len(dictionary.word2idx), n_tags=len(dictionary.word2idx), embedding_dim=hp.embedding_dim,\n",
    "                  n_hidden_rnn=hp.n_hidden_rnn, PAD_index=dictionary.word2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model_checkpoint = './model.chkpt'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    def __init__(self,K):\n",
    "        self.state_c=[]\n",
    "        self.state_h=[]\n",
    "        self.probabilities=[]\n",
    "        self.lastOutput=[]\n",
    "        self.history=[]\n",
    "        self.K=K\n",
    "    def addBeam(self,lastOutput,prob,state_c,state_h,history):\n",
    "        self.lastOutput.append(lastOutput)\n",
    "        self.probabilities.append(prob)\n",
    "        self.state_c.append(state_c)\n",
    "        self.state_h.append(state_h)\n",
    "        current_hist=history.copy()\n",
    "        current_hist.extend(lastOutput)\n",
    "        self.history.append(current_hist)\n",
    "    def getTopK(self):\n",
    "        topK=np.argsort(self.probabilities)[-self.K:]\n",
    "        lenTop=len(topK)\n",
    "        state_c,state_h=np.zeros((lenTop,200)),np.zeros((lenTop,200))\n",
    "        prob=[]\n",
    "        lastOutput=[]\n",
    "        history=[]\n",
    "        for i,k in enumerate(topK):\n",
    "            lastOutput.append(self.lastOutput[k])\n",
    "            history.append(self.history[k])\n",
    "            prob.append(self.probabilities[k])\n",
    "            state_c[i,:]=self.state_c[k]\n",
    "            state_h[i,:]=self.state_h[k]\n",
    "        return lastOutput,prob,state_c,state_h,history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 0], [2, 1, 1], [2, 1, 3]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Beam\n",
    "beam=Beam(3)\n",
    "vec=dictionary.text2vec('Hi')\n",
    "prob=1\n",
    "state_c,state_h=np.zeros((1,200)),np.zeros((1,200))\n",
    "beam.addBeam(vec+[0],0.8,state_c,state_h,[])\n",
    "beam.addBeam(vec+[1],0.9,state_c+1,state_h+1,[])\n",
    "beam.addBeam(vec+[2],0.1,state_c+2,state_h+2,[])\n",
    "beam.addBeam(vec+[3],0.96,state_c+3,state_h+3,[])\n",
    "lastOutputs,probs,state_c,state_h,history=beam.getTopK()\n",
    "\n",
    "assert (lastOutputs[0][2]==0 and lastOutputs[1][2]==1 and lastOutputs[2][2]==3)\n",
    "assert probs==[0.8, 0.9, 0.96]\n",
    "assert state_c[0,0]==0 and state_c[1,0]==1 and state_c[2,0]==3 \n",
    "assert state_h[0,0]==0 and state_h[1,0]==1 and state_h[2,0]==3 \n",
    "lastOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modification\n",
    "# add session,model to parameters\n",
    "def beam_search(num_generated,seed):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "            num_generated: number of tokens to be generated.\n",
    "            seed: initial sentence of the text.\n",
    "            K: default value is 3;  #number of sequences to track\n",
    "    return: topK candidates of the generated text\n",
    "    \"\"\"\n",
    "\n",
    "    beam=Beam(hp.K)\n",
    "    vec=dictionary.text2vec(seed)  # 1xL \n",
    "    state_c,state_h=np.zeros((1,200)),np.zeros((1,200)) # initial states\n",
    "    p=1\n",
    "    beam.addBeam(vec,p,state_c,state_h,[])\n",
    "\n",
    "    for i in range(num_generated):\n",
    "        lastOutputs,probs,state_c,state_h,history=beam.getTopK()\n",
    "\n",
    "        softmax,states=model.predict_for_batch(sess,lastOutputs,state_c,state_h) # softmax [NONE,Len,10000]\n",
    "                                                                                 # states ([NONE,200],[NONE,200])\n",
    "        state_c,state_h=states\n",
    "\n",
    "        beam=Beam(hp.K)\n",
    "        topK=np.argsort(softmax[:,-1])[:,-hp.K:] #[None,K]\n",
    "        for i in range(topK.shape[0]):\n",
    "            for j in range(hp.K):\n",
    "                cand=topK[i,j]\n",
    "                if cand==1:continue\n",
    "                vec=[cand]\n",
    "                st_c=state_c[i,:]\n",
    "                st_h=state_h[i,:]\n",
    "                p=softmax[i,-1,cand]*probs[i]\n",
    "                hist=history[i]\n",
    "                beam.addBeam(vec,p,st_c,st_h,hist)\n",
    "    lastOutputs,probs,state_c,state_h,history=beam.getTopK()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def getBestCandidate(num_tokens,seed):\n",
    "    history=beam_search(num_tokens,seed)\n",
    "    return dictionary.vec2text(history[-1])\n",
    "#getBestCandidate(500,'just')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ُEboch 500/500.batch 1/1 \n",
      "Epoch 0: <START> How graced JOHN silence enemies unmoan caitiff unmoan unmoan for kings\n",
      "Epoch 10: <START> How #tab #tab #tab #tab #tab #tab #tab #tab #tab #tab\n",
      "Epoch 20: <START> How #tab #tab #tab #tab #tab #tab #tab #tab #tab #tab\n",
      "Epoch 30: <START> How #tab ( : ) #tab ( : ) #tab (\n",
      "Epoch 40: <START> How #tab ( : ) #tab ( of : ) #tab\n",
      "Epoch 50: <START> How #tab ( : ) #tab ( : ) #tab (\n",
      "Epoch 60: <START> How #tab ( Ghost of . ( : ) #tab (\n",
      "Epoch 70: <START> How #tab ( Ghost of Edward : ) #tab ( Ghost\n",
      "Epoch 80: <START> How #tab ( Ghost of York . ( Ghost of York\n",
      "Epoch 90: <START> How #tab ( Second Citizen : ) #tab ( Ghost of\n",
      "Epoch 100: <START> How #tab afterwards King Edward IV . ( QUEEN MARGARET :\n",
      "Epoch 110: <START> How #tab ( Ghost of York . ( QUEEN MARGARET :\n",
      "Epoch 120: <START> How #tab ( Ghost of Wales , ( RICHMOND : )\n",
      "Epoch 130: <START> How #tab ( Ghost of York . ( QUEEN MARGARET :\n",
      "Epoch 140: <START> How #tab ( Ghost of York . ( Boy : )\n",
      "Epoch 150: <START> How #tab KING RICHARD III : ) #tab ( Ghost of\n",
      "Epoch 160: <START> How #tab KING RICHARD III #tab ( Ghost of Wales ,\n",
      "Epoch 170: <START> How #tab KING RICHARD III : ) #tab ( Ghost of\n",
      "Epoch 180: <START> How #tab KING RICHARD III #tab ( Ghost of Wales ,\n",
      "Epoch 190: <START> How #tab KING RICHARD III : ) Called also EARL of\n",
      "Epoch 200: <START> How #tab KING RICHARD III #tab ( KING EDWARD : )\n",
      "Epoch 210: <START> How #tab KING RICHARD III #tab Duke of York , (\n",
      "Epoch 220: <START> How #tab KING RICHARD III #tab ( KING EDWARD : )\n",
      "Epoch 230: <START> How #tab KING RICHARD III : ) #tab | Brothers to\n",
      "Epoch 240: <START> How #tab KING EDWARD IV : ) #tab | Brothers to\n",
      "Epoch 250: <START> How #tab KING RICHARD III #tab ( TYRREL : ) #tab\n",
      "Epoch 260: <START> How #tab KING RICHARD III #tab ( Ghost of Wales ,\n",
      "Epoch 270: <START> How #tab KING RICHARD III #tab ( Ghost of Clarence ,\n",
      "Epoch 280: <START> How #tab KING RICHARD III : ) #tab | Brothers to\n",
      "Epoch 290: <START> How #tab KING RICHARD III #tab Duke of Clarence , (\n",
      "Epoch 300: <START> How #tab KING RICHARD III #tab ( KING EDWARD : )\n",
      "Epoch 310: <START> How #tab KING RICHARD III #tab #tab | the King .\n",
      "Epoch 320: <START> How #tab KING RICHARD III #tab Duke of Wales , (\n",
      "Epoch 330: <START> How KING RICHARD III #tab ( RATCLIFF : ) #tab |\n",
      "Epoch 340: <START> How KING EDWARD #tab ( KING EDWARD IV : ) #tab\n",
      "Epoch 350: <START> How #tab KING RICHARD III #tab ( Ghost of Prince Edward\n",
      "Epoch 360: <START> How KING RICHARD III #tab ( RATCLIFF : ) #tab |\n",
      "Epoch 370: <START> How KING EDWARD #tab ( KING EDWARD IV : ) #tab\n",
      "Epoch 380: <START> How #tab KING RICHARD III : ) #tab #tab | the\n",
      "Epoch 390: <START> How #tab KING RICHARD III #tab #tab | the King .\n",
      "Epoch 400: <START> How KING RICHARD III #tab Duke of Clarence , ( YORK\n",
      "Epoch 410: <START> How KING RICHARD III #tab Duke of Clarence , ( CLARENCE\n",
      "Epoch 420: <START> How KING RICHARD III #tab ( KING EDWARD IV : )\n",
      "Epoch 430: <START> How KING EDWARD #tab Prince of Wales , ( PRINCE EDWARD\n",
      "Epoch 440: <START> How KING RICHARD III #tab Duke of Gloucester , ( GLOUCESTER\n",
      "Epoch 450: <START> How KING RICHARD III #tab Duke of Gloucester , ( GLOUCESTER\n",
      "Epoch 460: <START> How KING RICHARD III #tab Duke of Gloucester , ( GLOUCESTER\n",
      "Epoch 470: <START> How KING RICHARD III #tab Duke of Clarence , ( CLARENCE\n",
      "Epoch 480: <START> How KING RICHARD III #tab Duke of Gloucester , ( GLOUCESTER\n",
      "Epoch 490: <START> How #tab KING RICHARD III : ) #tab #tab #tab |\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "learning_rate=hp.learning_rate\n",
    "print('Start training... \\n')\n",
    "results=\"\"\n",
    "for epoch in range(hp.n_epochs):\n",
    "    if epoch%10==0:\n",
    "        newc=getBestCandidate(10,\"How\")\n",
    "        results+=\"\\nEpoch {}: {}\".format(epoch,newc)\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(hp.n_epochs) + '-' * 20+results)\n",
    "    \n",
    "    counter=0\n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(hp.batch_size, dataset,dictionary):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        counter=counter+1\n",
    "        print(\"ُEboch {}/{}.batch {}/{} {}\".format(epoch+1,hp.n_epochs,counter,len(dataset)//hp.batch_size,results))\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, hp.dropout_keep_probability)\n",
    "    saver.save(sess, model_checkpoint, global_step=epoch,write_meta_graph=False)\n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / hp.learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.3 µs\n",
      "<START> How KING RICHARD III #tab Duke of Gloucester , ( GLOUCESTER : ) #tab | Brothers to #tab #tab | ( GREY #tab ( GREY : ) #tab | ( Gentleman : ) Called also EARL of SURREY #tab His son . ( SURREY : ) Called also EARL of SURREY #tab His son . ( SURREY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor of Wales , ( PRINCE EDWARD : ) #tab | Sons to Elizabeth . ( RIVERS #tab ( GREY : ) Called also EARL of DERBY . ( DERBY : ) Called also EARL of DERBY . ( SURREY : ) Called also EARL of DERBY . ( DERBY : ) #tab | Gentlemen attending on the lascivious pleasing of a lute . ( BRAKENBURY : ) #tab | Gentlemen attending on the Lady Anne . ( Lord Mayor : ) #tab | ( Gentleman : ) #tab | ( Gentleman : ) #tab | ( Gentleman : ) #tab | ( Gentleman : ) #tab | ( Gentleman : )\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Generate text of 1000 words\n",
    "print(getBestCandidate(1000,'How'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo\n",
    "# version 1\n",
    "# check inference\n",
    "# check beam-search\n",
    "\n",
    "# version2\n",
    "# Evaluation\n",
    "\n",
    "# char-level\n",
    "\n",
    "# Russian\n",
    "# Arabic \n",
    "# French"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
