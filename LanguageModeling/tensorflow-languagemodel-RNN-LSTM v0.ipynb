{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 0\n",
    "\n",
    "500 iterations\n",
    "\n",
    "44 docs each of length 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ikntcit1/shaheen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from lib.dictionarymd import Dictionary\n",
    "from lib.textprocessingmd import convert_text\n",
    "from lib.fileoperationmd import getFilesFromPath,readTxtFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    DATAPATH='./data/shakespeare/'\n",
    "    VOCPATH='./preprocessed'\n",
    "    VOCFILE='articles.voc'\n",
    "\n",
    "    MAXLENGTH=10000  # MAX Length of the document\n",
    "hp=HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=getFilesFromPath(hp.DATAPATH)\n",
    "dataset=[]\n",
    "text=\"\"\n",
    "for fn in filenames:\n",
    "    ntext=convert_text(readTxtFromFile(hp.DATAPATH,fn))\n",
    "    text+=ntext\n",
    "    dataset.append(ntext)\n",
    "    break\n",
    "\n",
    "dictionary=Dictionary()\n",
    "dictionary.make_vocab(text,hp.VOCPATH,hp.VOCFILE)\n",
    "dictionary.load_vocab(hp.VOCPATH,hp.VOCFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can understand from this code that multiple batches maybe with different maxlen\n",
    "def batches_generator(batch_size, docs,dictionary,\n",
    "                      shuffle=False, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(docs)\n",
    "    vecs=[dictionary.text2vec(doc)[:hp.MAXLENGTH] for doc in docs]\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        lengths=[len(s) for s in vecs]\n",
    "        order = np.argsort(lengths)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(vecs[idx][:-1])\n",
    "            y_list.append(vecs[idx][1:])\n",
    "            max_len_token = max(max_len_token, len(vecs[idx]))  #why?!\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * dictionary.word2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * dictionary.word2idx['<PAD>']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        \n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10000) (1, 10000) (1,) [9999]\n"
     ]
    }
   ],
   "source": [
    "for x,y,lengths in batches_generator(1, dataset[:2],dictionary):\n",
    "    print(x.shape,y.shape,lengths.shape,lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/intel/intelpython3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "  \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, dtype=tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, dtype=tf.float32, name='embeddings_matrix')\n",
    "\n",
    "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "    #backward_cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                                                  \n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "\n",
    "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "    rnn_output, _ = tf.nn.dynamic_rnn(cell=forward_cell, sequence_length=self.lengths, dtype=tf.float32, inputs=embeddings)\n",
    "    #rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "    \n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, vocabulary_size, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "    \n",
    "    # Create softmax (tf.nn.softmax) function\n",
    "    self.softmax_output = tf.nn.softmax(logits=self.logits)\n",
    "    \n",
    "    # Use argmax (tf.argmax) to get the most probable tags\n",
    "    # Don't forget to set axis=-1\n",
    "    # otherwise argmax will be calculated in a wrong way\n",
    "    self.predictions = tf.argmax(self.softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, vocabulary_size, PAD_index):\n",
    "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "    \n",
    "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, vocabulary_size)\n",
    "    #loss_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=ground_truth_tags_one_hot)\n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), dtype=tf.float32)\n",
    "    loss_tensor = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits=self.logits,\n",
    "        targets=self.ground_truth_tags,\n",
    "        weights=mask)\n",
    "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "    # Be careful that the argument of tf.reduce_mean should be\n",
    "    # multiplication of mask and loss_tensor.\n",
    "    self.loss = tf.reduce_mean(loss_tensor)#np.multiply(mask, loss_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    # Create an optimizer (tf.train.AdamOptimizer)\n",
    "    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "    # Pay attention that you need to apply this operation only for gradients \n",
    "    # because self.grads_and_vars contains also variables.\n",
    "    # list comprehension might be useful in this case.\n",
    "    clip_norm = tf.cast(1.0, dtype=tf.float32)  ##??\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.lengths: lengths}\n",
    "    k=3\n",
    "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
    "    softmax = session.run(self.softmax_output, feed_dict=feed_dict)\n",
    "    topk=softmax.argsort()[:,:,-k:]\n",
    "    topkp=softmax[:,:,topk]\n",
    "    return topk,topkp,softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = LSTMModel(vocabulary_size=len(dictionary.word2idx), n_tags=len(dictionary.word2idx), embedding_dim=200,\n",
    "                  n_hidden_rnn=200, PAD_index=dictionary.word2idx['<PAD>'])\n",
    "\n",
    "batch_size = 1\n",
    "n_epochs = 6\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 1.41\n",
    "dropout_keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ُEboch 500/500.batch 1/1\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "learning_rate=0.01\n",
    "n_epochs=500\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    \n",
    "    counter=0\n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, dataset,dictionary):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        counter=counter+1\n",
    "        print(\"ُEboch {}/{}.batch {}/{}\".format(epoch+1,n_epochs,counter,len(dataset)//batch_size))\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "    saver.save(sess, './my-model', global_step=epoch,write_meta_graph=False)\n",
    "    # Decaying the learning rate\n",
    "    #learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "### input: \n",
    "- k \n",
    "\n",
    "\n",
    "- C: candidates number k\n",
    "- P: probabilities Pi is probability of candidate Ci\n",
    "\n",
    "Extend candidates\n",
    "\n",
    "- for each candidate:\n",
    "    - find k new candidates out of current candidate\n",
    "    - add them to C and probabilties to P\n",
    "    - keep top k candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def beamSearch(k,seed,dictionary,L=250):\n",
    "    C=[[dictionary.word2idx[token] for token in seed.split()]]\n",
    "    start=len(C[0])\n",
    "    oldC=[]\n",
    "    oldP=[]\n",
    "    P=[1]\n",
    "    for length in range(start,L):\n",
    "        clear_output(wait=True)\n",
    "        print(length)\n",
    "        clength=np.array([length])\n",
    "        oldC.append(C)\n",
    "        oldP.append(P)\n",
    "        newC=[]\n",
    "        newP=[]\n",
    "        for ix,x in enumerate(C):\n",
    "            x_batch=np.zeros((1,len(x)),dtype=np.int32)\n",
    "            x_batch[0,:]=x\n",
    "            retk,retp,softmax=model.predict_for_batch(sess,x_batch,clength)\n",
    "            retk=retk[0,length-1]\n",
    "            softmax=softmax[0,length-1,retk]\n",
    "            \n",
    "            for iidx,idx in enumerate(retk):\n",
    "                #if idx!=dictionary.word2idx['<UNK>'] :#and idx!=dictionary.word2idx['#endl']:\n",
    "                    newC.append(x+[idx])\n",
    "                    newP.append(P[ix]*softmax[iidx])\n",
    "        #perplexities=np.array(newP,-l)\n",
    "        argsort=np.argsort(newP)[-3:]\n",
    "        P,C=[],[]\n",
    "        for ias in argsort:\n",
    "            P.append(newP[ias])\n",
    "            C.append(newC[ias])\n",
    "        rands=[]\n",
    "        for i in range(3):\n",
    "            rand=random.randint(0,len(newP)-1)        \n",
    "            if length-start>4:\n",
    "                while rand in argsort or rand in rands:\n",
    "                    rand=random.randint(0,len(newP)-1)\n",
    "            rands.append(rand)\n",
    "            P.append(newP[rand])\n",
    "            C.append(newC[rand])\n",
    "\n",
    "    return oldC,oldP,C[2],P[2]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "Generated text:\n",
      "--------------------------------------------------\n",
      "we would blot thee . <UNK> <UNK> QUEEN ELINOR #tab Nay , I would not wish a better father . <UNK> #tab Some sins do bear their privilege on earth , <UNK> #tab And so doth yours ; your fault was not your folly : <UNK> #tab Needs must you lay your heart at his dispose , <UNK> #tab Subjected tribute to commanding love , <UNK> #tab Against whose fury and unmatched force <UNK> #tab The aweless lion could not wage the fight , <UNK> #tab Nor keep his princely heart from Richard ' s a large mouth , <UNK> #tab That spits forth death and mountains , rocks and seas , <UNK> #tab Talks as familiarly of roaring lions <UNK> #tab As maids of thirteen do of puppy - dogs ! <UNK> #tab What cannoneer begot this lusty blood ? <UNK> #tab He speaks plain cannon fire , and smoke and bounce ; <UNK> #tab But from the inward motion to deliver <UNK> #tab Sweet , sweet , sweet poison for the age ' s tooth : <UNK> #tab Which fault lies on the hazards of all husbands <UNK> #tab That marry wives . Tell me , how if my brother ' s plea and none of mine ; <UNK> #tab The which if he can prove , a ' pops me out <UNK> #tab At least from fair five hundred pound a year , <UNK> #tab Yet sell your face for five pence and ' tis dear .\n"
     ]
    }
   ],
   "source": [
    "oldC,oldP,C,P=beamSearch(1,'we',dictionary)\n",
    "generated=dictionary.vec2text(C)\n",
    "print(\"Generated text:\\n\"+'-'*50+'\\n'+generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "Generated text:\n",
      "--------------------------------------------------\n",
      "these days we command city city saucy thee each strew never give him strength <UNK> #tab To make room for him in peace permit <UNK> #tab Our just and lineal entrance to our own ; <UNK> #tab If lusty love should go in search of virtue , <UNK> #tab Where should he find it purer than in Blanch ? <UNK> #tab If love ambitious sought a match of birth , <UNK> #tab Whose sons lie scattered on the bleeding ground ; <UNK> #tab Many a widow ' s husband grovelling lies , <UNK> #tab Coldly embracing the discolour ' d earth ; <UNK> #tab And victory , with little loss , doth play <UNK> #tab Upon the dancing banners of the French , <UNK> #tab That holds in chase mine honour up and down ? <UNK> <UNK> BASTARD #tab Brother , adieu : good fortune come to thee ! <UNK> <UNK> BASTARD #tab Brother , adieu : good fortune come to thee ! <UNK> <UNK> KING JOHN #tab Acknowledge then the king , and partly in France . <UNK> <UNK> #tab [ Enter KING JOHN , QUEEN ELINOR , PEMBROKE , ESSEX , <UNK> #tab SALISBURY , and others , with CHATILLON ] <UNK> <UNK> KING JOHN #tab A wonder , lady ! lo , upon thy wish , <UNK> #tab Our messenger Chatillon is arrived ! <UNK> #tab What England says , say briefly , gentle lord ; <UNK> #tab Our cannons ' malice vainly shall be spent <UNK> #tab Against the invulnerable clouds of heaven ; <UNK> #tab And two such shores to two such streams made one , <UNK> #tab Two such controlling bounds shall you be , kings , <UNK> #tab To these two princes , if you marry them . <UNK> #tab This union shall do more than battery can <UNK> #tab To make a shaking fever in your walls , <UNK> #tab They shoot but calm words folded up in smoke , <UNK> #tab To make a faithless error in your ears : <UNK> #tab Which trust accordingly , kind citizens , <UNK> #tab That judge hath made me guardian to this boy : <UNK> #tab Under whose warrant I impeach thy wrong <UNK> #tab And confident from foreign purposes , <UNK> #tab Even till that utmost corner of the west <UNK> #tab Salute thee for her king : till then , fair boy , <UNK> #tab And I shall show you peace and fair - faced league ; <UNK> #tab Win you this city without stroke or wound ; <UNK> #tab Rescue those breathing lives to die in beds , <UNK> #tab That here come sacrifices for the field : <UNK> #tab That yon green boy shall have no sun to ripe <UNK> #tab The bloom that promiseth a mighty fruit . <UNK> #tab I see a yielding in the looks of France ; <UNK> #tab Mark , how they whisper : urge them while their souls <UNK> #tab Are capable of this ambition , <UNK> #tab Lest zeal , now melted by the windy breath <UNK> #tab Of soft petitions , pity and remorse , <UNK> #tab To make hazard of new fortunes here : <UNK> #tab In brief , a braver choice of dauntless spirits <UNK> #tab Than now the English bottoms have waft o ' er <UNK> #tab Did nearer float upon the swelling tide , <UNK> #tab To make a faithless error in your ears : <UNK> #tab Which trust accordingly , kind citizens , <UNK> #tab That judge hath made me guardian to this boy : <UNK> #tab To save unscratch ' d your city ' s threatened cheeks , <UNK> #tab Behold , the French amazed vouchsafe a parle ; <UNK> #tab And when that we have dash ' d them to the ground , <UNK> #tab That water - walled bulwark , still secure <UNK> #tab And confident from foreign purposes , <UNK> #tab Even till that utmost corner of the west <UNK> #tab Salute thee for her king : till then , fair boy , <UNK> #tab And then comes answer like an Absey book : <UNK> #tab For this down - trodden equity , we tread <UNK> #tab In warlike march these greens before your town , <UNK> #tab Being no further enemy to you <UNK> #tab Than the constraint of hospitable zeal <UNK> #tab In the relief of this oppressed child <UNK> #tab Religiously provokes . Be pleased then <UNK> #tab To make hazard of new fortunes here : <UNK> #tab In brief , a braver choice of dauntless spirits <UNK> #tab Than now the English bottoms have waft o ' er <UNK> #tab Did nearer float upon the swelling tide , <UNK> #tab To make a faithless error in your ears : <UNK> #tab Which trust accordingly , kind citizens , <UNK> #tab That judge hath made me guardian to this boy : <UNK> #tab Under whose warrant I impeach thy wrong <UNK> #tab And confident from foreign purposes , <UNK> #tab Even till that utmost corner of the west <UNK> #tab Salute thee for her king : till then , fair boy , <UNK> #tab And I shall show you peace and fair - faced league ; <UNK> #tab Win you this city without stroke or wound ; <UNK> #tab Rescue those breathing lives to die in beds , <UNK> #tab That here come sacrifices for the field : <UNK> #tab That yon green boy shall have no sun to ripe <UNK> #tab The bloom that promiseth a mighty fruit . <UNK> #tab I see a yielding in the looks of France ; <UNK> #tab Mark , how they whisper : urge them while their souls <UNK> #tab Are capable of this ambition , <UNK> #tab Lest zeal , now melted by the windy breath <UNK> #tab Of soft petitions , pity and remorse , <UNK> #tab To make a shaking fever in your walls , <UNK> #tab They shoot but\n"
     ]
    }
   ],
   "source": [
    "oldC,oldP,C,P=beamSearch(1,'these days we',dictionary,1000)\n",
    "generated=dictionary.vec2text(C)\n",
    "print(\"Generated text:\\n\"+'-'*50+'\\n'+generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "# train on full documents\n",
    "# char-level\n",
    "# check inference\n",
    "# check beam-search\n",
    "# arabic \n",
    "# french\n",
    "# russian\n",
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
