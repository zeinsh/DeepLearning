{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version(1.3): \n",
    "\n",
    "- Use bbc-dataset\n",
    "- try to fix save/load model\n",
    "\n",
    "In the upcoming versions:\n",
    "- Evaluation using perplexity\n",
    "- Beam search, randomize to not repeat the same sentence\n",
    "- apply to other languages\n",
    "    - Russian\n",
    "    - Arabic\n",
    "    - French\n",
    "- apply this network for char-level language modeling.\n",
    "    \n",
    "For restoring previous check-point, I was forced to define the whole model in this notebook, rather that importing it from an external .py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lib.dictionarymd import Dictionary\n",
    "from lib.textprocessingmd import convert_text\n",
    "from lib.fileoperationmd import getFilesFromPath,readTxtFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyper import HyperParameters\n",
    "hp=HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.txt not opened\n"
     ]
    }
   ],
   "source": [
    "filenames=getFilesFromPath(hp.DATAPATH)\n",
    "dataset=[]\n",
    "text=\"\"\n",
    "for fn in filenames:\n",
    "    ntext=convert_text(readTxtFromFile(hp.DATAPATH,fn))\n",
    "    text+=ntext\n",
    "    dataset.append(ntext)\n",
    "\n",
    "dictionary=Dictionary()\n",
    "dictionary.make_vocab(text,hp.VOCPATH,hp.VOCFILE)\n",
    "dictionary.load_vocab(hp.VOCPATH,hp.VOCFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zein/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from RNN_LSTM_LM import LSTMModel\n",
    "from helpers import batches_generator\n",
    "from helpers import Beam\n",
    "from helpers import beam_search,getBestCandidate\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modifications\n",
    "# 1. add training/inference variable\n",
    "# 2. define placeholder for state [NONE,2*stateLength]\n",
    "# 3. if inference use state placeholder\n",
    "# 4. modify beam search\n",
    "# 5. try to use built-in beam search\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from hyper import HyperParameters\n",
    "hp=HyperParameters()\n",
    "\n",
    "class LSTMModel():\n",
    "    def __declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "        # Placeholders for input and ground truth output.\n",
    "        self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "        self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
    "        self.initial_state_h= tf.placeholder(dtype=tf.float32, shape=[None, hp.n_hidden_rnn], name='initial_state_h')\n",
    "        self.initial_state_c= tf.placeholder(dtype=tf.float32, shape=[None, hp.n_hidden_rnn], name='initial_state_c')\n",
    "\n",
    "        # Placeholder for lengths of the sequences.\n",
    "        self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "\n",
    "        # Placeholder for a dropout keep probability. If we don't feed\n",
    "        # a value for this placeholder, it will be equal to 1.0.\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, dtype=tf.float32), shape=[])\n",
    "\n",
    "        # Placeholder for a learning rate (tf.float32).\n",
    "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph')\n",
    "\n",
    "    def __build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "        \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "\n",
    "        # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "        initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "        embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, dtype=tf.float32, name='embeddings_matrix')\n",
    "\n",
    "        # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
    "        # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
    "\n",
    "        forward_cell =  tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn),\n",
    "                                                      input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "        #backward_cell = tf.nn.rnn_cell.DropoutWrapper(cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "\n",
    "        # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "        # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "        embeddings = tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "\n",
    "        # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "        # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "        # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "        rnn_output, self.states = tf.nn.dynamic_rnn(cell=forward_cell,\n",
    "                                          initial_state=tf.nn.rnn_cell.LSTMStateTuple(self.initial_state_c, self.initial_state_h),\n",
    "                                          sequence_length=self.lengths,\n",
    "                                          dtype=tf.float32,\n",
    "                                          inputs=embeddings)\n",
    "        #rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "        # Dense layer on top.\n",
    "        # Shape: [batch_size, sequence_len, n_tags].   \n",
    "        self.logits = tf.layers.dense(rnn_output, vocabulary_size, activation=None)\n",
    "\n",
    "    def __compute_predictions(self):\n",
    "        \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "\n",
    "        # Create softmax (tf.nn.softmax) function\n",
    "        self.softmax_output = tf.nn.softmax(logits=self.logits)\n",
    "\n",
    "        # Use argmax (tf.argmax) to get the most probable tags\n",
    "        # Don't forget to set axis=-1\n",
    "        # otherwise argmax will be calculated in a wrong way\n",
    "        self.predictions = tf.argmax(self.softmax_output, axis=-1)\n",
    "        \n",
    "    def __compute_loss(self, vocabulary_size, PAD_index):\n",
    "        \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "\n",
    "        # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
    "        ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, vocabulary_size)\n",
    "        #loss_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=ground_truth_tags_one_hot)\n",
    "        mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), dtype=tf.float32)\n",
    "        loss_tensor = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits=self.logits,\n",
    "            targets=self.ground_truth_tags,\n",
    "            weights=mask)\n",
    "        # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
    "        # Be careful that the argument of tf.reduce_mean should be\n",
    "        # multiplication of mask and loss_tensor.\n",
    "        self.loss = tf.reduce_mean(loss_tensor)#np.multiply(mask, loss_tensor))\n",
    "        \n",
    "    def __perform_optimization(self):\n",
    "        \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "\n",
    "        # Create an optimizer (tf.train.AdamOptimizer)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "\n",
    "        # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
    "        # Pay attention that you need to apply this operation only for gradients \n",
    "        # because self.grads_and_vars contains also variables.\n",
    "        # list comprehension might be useful in this case.\n",
    "        clip_norm = tf.cast(1.0, dtype=tf.float32)  ##??\n",
    "        self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "\n",
    "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
    "        \n",
    "    def __init__(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "        self.__declare_placeholders()\n",
    "        self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "        self.__compute_predictions()\n",
    "        self.__compute_loss(n_tags, PAD_index)\n",
    "        self.__perform_optimization()\n",
    "        \n",
    "    def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {self.input_batch: x_batch,\n",
    "                     self.ground_truth_tags: y_batch,\n",
    "                     self.initial_state_h: np.zeros((lengths.shape[0],hp.n_hidden_rnn)),\n",
    "                     self.initial_state_c: np.zeros((lengths.shape[0],hp.n_hidden_rnn)),\n",
    "                     self.learning_rate_ph: learning_rate,\n",
    "                     self.dropout_ph: dropout_keep_probability,\n",
    "                     self.lengths: lengths}\n",
    "\n",
    "        session.run(self.train_op, feed_dict=feed_dict)\n",
    " \n",
    "    def predict_for_batch(self, session, x_batch,init_c,init_h):\n",
    "        lengths=np.array([1000000]*len(x_batch))\n",
    "        feed_dict = {self.input_batch: x_batch,\n",
    "                     self.initial_state_h: init_h,\n",
    "                     self.initial_state_c: init_c,\n",
    "                     self.lengths: lengths}\n",
    "        k=3\n",
    "        softmax, states = session.run([self.softmax_output ,self.states], feed_dict=feed_dict)\n",
    "        return softmax, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zein/.local/lib/python3.5/site-packages/tensorflow/python/ops/clip_ops.py:113: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(vocabulary_size=len(dictionary.word2idx), n_tags=len(dictionary.word2idx), embedding_dim=hp.embedding_dim,\n",
    "                  n_hidden_rnn=hp.n_hidden_rnn, PAD_index=dictionary.word2idx['<PAD>'])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#model_checkpoint = './model.chkpt'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ُEboch 50/50.batch 511/511 \n",
      "Epoch 49: <START> I ' ve got to be able to be a lot of chances , \" he said <UNK> \" It ' s a lot of people have a lot of chances , \" he said <UNK> \" It ' s a lot of chances , \" he told BBC Radio Five Live <UNK> \" It ' s not going to be a good game <UNK> \" I ' m sure it ' s a lot of chances , \" he added <UNK> \" I ' m not going to play <UNK> \" I ' m not going to be a lot of people <UNK> \" I ' m not going to be a lot of chances , \" he told BBC Sport <UNK> \" I ' m not going to be able to be a lot of chances <UNK> \" I ' m not going to be a lot of people\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "learning_rate=hp.learning_rate\n",
    "print('Start training... \\n')\n",
    "results=\"\"\n",
    "start=16\n",
    "for epoch in range(start,hp.n_epochs):\n",
    "    newc=getBestCandidate(sess,model,150,\"I\",dictionary)\n",
    "    results=\"\\nEpoch {}: {}\".format(epoch,newc)\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    #print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(hp.n_epochs) + '-' * 20+results)\n",
    "    \n",
    "    counter=0\n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(hp.batch_size, dataset,dictionary):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        counter=counter+1\n",
    "        print(\"ُEboch {}/{}.batch {}/{} {}\".format(epoch+1,hp.n_epochs,counter,len(dataset)//hp.batch_size,results))\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, hp.dropout_keep_probability)\n",
    "    # Decaying the learning rate\n",
    "    saver.save(sess, hp.MODEL_CHKPNT_PATH,global_step=epoch)\n",
    "    learning_rate = learning_rate / 1.61#hp.learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.2 µs\n",
      "<START> The pair are facing lengthy bans for the missed <UNK> including the game <UNK> \" I ' m not going to be able to be able to be able to be able to play <UNK> \" I ' m not going to be able to be able to get the game <UNK> \" I ' ve got a lot of people who has been a good game <UNK> \" I ' m not going to be a lot of chances <UNK> \" It ' s a lot of people who have a good game , \" said <UNK> \" I ' ve got to get a lot of chances , \" he said <UNK> \" I ' m sure it ' s a lot of chances , \" he said <UNK> \" It ' s not going to be a lot of people <UNK> \" I ' m not going to be able to be a lot of people <UNK> \" I ' ve got to play on the pitch <UNK> \" I ' m not going to be able to be able to play <UNK> \" I ' m not going to be a lot of chances <UNK> \" I ' m not going to be a good game <UNK> \" I ' m sure it ' s a lot of chances , \" he said <UNK> \" I ' ve got to be a lot of people who has been a lot of people who has been a lot of people and I ' m not going to play <UNK> \" I ' ve got to play on the pitch , \" he said <UNK> \" It ' s a great opportunity to get the game <UNK> \" I ' m not going to be able to get a lot of chances <UNK> \" It ' s a lot of people who have a lot of people who has been a lot of chances <UNK> \" It ' s a lot of chances , \" he said <UNK> \" I ' m not going to be a lot of people <UNK> \" I ' m sure it ' s a lot of chances , \" he said <UNK> \" I ' m not going to be a lot of chances <UNK> \" I ' m not going to be a lot of chances , \" he said <UNK> \" It ' s a lot of people who have been in the game , \" he said <UNK> \" It ' s a lot of chances , \" he said <UNK> \" It ' s a lot of people who have been a lot of people who is a lot of chances , \" he said <UNK> \" It ' s a great opportunity to be a lot of chances , \" he told BBC Sport <UNK> \" I ' ve got to be a lot of chances , \" he said <UNK> \" It ' s very disappointing , \" he said <UNK> \" It ' s a great\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Generate text of 1000 words\n",
    "full=\"\"\"The pair are facing lengthy bans for the missed tests, including one on the eve of last year's Athens Olympics. \n",
    "They were set to learn their fate by the end of February, but late evidence from them has pushed the date back. \"A decision is now expected by around mid-March,\" said one of their lawyers, Michalis Dimitrakopoulos.\"\"\"\n",
    "print(getBestCandidate(sess,model,500,'The pair are facing lengthy bans for the missed tests, including',dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restore check-point\n",
    "tf.reset_default_graph()  \n",
    "model = LSTMModel(vocabulary_size=len(dictionary.word2idx), n_tags=len(dictionary.word2idx), embedding_dim=hp.embedding_dim,\n",
    "                  n_hidden_rnn=hp.n_hidden_rnn, PAD_index=dictionary.word2idx['<PAD>'])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "saver.restore(sess,hp.MODEL_CHKPNT_PATH+'-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo\n",
    "\n",
    "# version2\n",
    "# Evaluation\n",
    "\n",
    "# char-level\n",
    "\n",
    "# larger network\n",
    "\n",
    "# Russian\n",
    "# Arabic \n",
    "# French"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
